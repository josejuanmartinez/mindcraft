{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://github.com/josejuanmartinez/mindcraft/blob/main/galadriel.png?raw=true)"
      ],
      "metadata": {
        "id": "8sIp_xQrB4TU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2\n",
        "This is an example notebook to showcase:\n",
        "1. How to load a book as common lore and make a World out of it;\n",
        "2. How to create NPCs with personality features;\n",
        "3. How to load locally a quantized LLM and use it as the mind of the NPC, using **Paged Attention (Fast Inference)** thanks to vLLM;\n",
        "\n",
        "We will be using the first book of The Lord of the Rings and Galadriel and an example."
      ],
      "metadata": {
        "id": "SEsWQ25afhu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PHI 6B and the quality of the output\n",
        "\n",
        "**vLLM** requires at least 16GB for serving and running fast inference on most of the average models. As the free version of Google Colab has exactly that amount (16GB), we will be running a very small size LLM, `Phi-6B`, so the results won't be as great as with the previous Example 1 notebook"
      ],
      "metadata": {
        "id": "V2U3r4kMtTs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download some example text from the Internet Archive"
      ],
      "metadata": {
        "id": "3Dfb2TblhyBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.org/download/j-r-r-tolkien-lord-of-the-rings-01-the-fellowship-of-the-ring-retail-pdf/j-r-r-tolkien-lord-of-the-rings-01-the-fellowship-of-the-ring-retail-pdf_djvu.txt -O lotr1.txt"
      ],
      "metadata": {
        "id": "Hn3kEN_3gzRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1cd2ded-f9aa-4f2f-f217-bd3b4340baed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Mindcraft\n",
        "This will install the library. Take into account that additional `pip installs` will be required as you select which Vector Store you want to use, what inference engine (Hugging Face, vLLM, etc)."
      ],
      "metadata": {
        "id": "b-tCDqxViZTW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83kv3zivGVvD"
      },
      "outputs": [],
      "source": [
        "!pip install mindcraft"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We will be using ChromaDB as a Vector Store, so I install it\n",
        "Vector Stores will keep all the knowledge of the world and the memories of our NPCs. Both will be used to tailor the interactions."
      ],
      "metadata": {
        "id": "KtDJFTOBinLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "m3DLdLDNI54g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing vLLM for Fast Inference\n",
        "```\n",
        "LLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with:\n",
        "\n",
        "- State-of-the-art serving throughput\n",
        "- Efficient management of attention key and value memory with PagedAttention\n",
        "- Continuous batching of incoming requests\n",
        "- Fast model execution with CUDA/HIP graph\n",
        "- Quantization: GPTQ, AWQ, SqueezeLLM\n",
        "- Optimized CUDA kernels\n",
        "\n",
        "vLLM is flexible and easy to use with:\n",
        "\n",
        "- Seamless integration with popular Hugging Face models\n",
        "- High-throughput serving with various decoding algorithms, including parallel sampling, beam search, and more\n",
        "- Tensor parallelism support for distributed inference\n",
        "- Streaming outputs\n",
        "- OpenAI-compatible API server\n",
        "- Support NVIDIA GPUs and AMD GPUs\n",
        "```\n",
        "\n",
        "**vLLM** seamlessly supports many Hugging Face models, including a lot of architectures. Check [here](https://github.com/vllm-project/vllm) for more information"
      ],
      "metadata": {
        "id": "TGvhFumH13Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm"
      ],
      "metadata": {
        "id": "yaJBqujv27-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chroma on Colab requires a restart!\n",
        "This will kill Colab. When it restarts, continue from next line."
      ],
      "metadata": {
        "id": "DElPXyfFi6bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chroma wants this session to be killed first\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "NT6iTGKnMoUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's recreate the `World` of Middle Earth!\n",
        "From the events described in our `lotr1.txt` that includes part of the novel.\n",
        "\n",
        "We will use:\n",
        "- A small Sentence Embeddings (`MINILM`) to encode texts to store in Chroma (lore and memories);\n",
        "- `ChromaDB` as our vector store;\n",
        "- `Zephyr7B_AWQ`, as our LLM to create NPCs answers.\n",
        "\n",
        "NOTE: A `World` can contain spaces and be from 3 to 63 character long, but don't include special characters or punctuation."
      ],
      "metadata": {
        "id": "IHdmUqvWjHOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "from mindcraft.infra.engine.llm_types import LLMType\n",
        "from mindcraft.infra.splitters.text_splitters_types import TextSplitterTypes\n",
        "from mindcraft.infra.vectorstore.stores_types import StoresTypes\n",
        "from mindcraft.infra.embeddings.embeddings_types import EmbeddingsTypes\n",
        "from mindcraft.lore.world import World\n",
        "\n",
        "world = World(world_name=\"Middle Earth from the Lord of the Rings\",\n",
        "              embeddings=EmbeddingsTypes.MINILM,\n",
        "              store_type=StoresTypes.CHROMA,\n",
        "              llm_type=LLMType.YI_6B_AWQ,\n",
        "              fast=True) # <--- fast=True to switch on vLLM"
      ],
      "metadata": {
        "id": "ScQ5ZDebHpD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We add the book as lore of the world.\n",
        "We will be supposing our NPC, Galadriel, knows everything happening in the world.\n",
        "\n",
        "If not, you could just split your lore by the characters and upload character-restricted lore (tutorial to be added soon!)"
      ],
      "metadata": {
        "id": "3iFdCM1OkHVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Books, specially txt from them, have very inconsistent format. For example,m this is how part of our book looks like:\n",
        "\n",
        "```\n",
        "the  detailed  index  of  names  promised  in  the  first  edition,\n",
        "but,  rather,  a  bald  index  with  only  names  and  page  refer¬\n",
        "ences.  Additionally,  at  this  time  the  appendices  were  greatly\n",
        "revised.\n",
        "```\n"
      ],
      "metadata": {
        "id": "ytlisu6bkqOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Splitting Approaches\n",
        "\n",
        "This means we need an intelligent way to split text into chunks.\n",
        "\n",
        "Right now we support 2 approaches:\n",
        "- `Sentence splitting`: Intelligently splitting by sentences and storing them.\n",
        "- `Chunk splitting`: Split the text based on a number of characters. This is not the best way as may break down sentences.\n",
        "\n",
        "We will be using `Sentence Splitting` in this tutorial."
      ],
      "metadata": {
        "id": "D6aT3njNlcva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Splitting\n",
        "To split by sentences, we need another dependency: `spacy`."
      ],
      "metadata": {
        "id": "xz-cDBquldWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.7.0"
      ],
      "metadata": {
        "id": "X9HXtS6ilyOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to CANCEL THE EXECUTION of the following line after a couple of minutes if you don't want to wait the whole lore to be loaded (final quality may be affected though)\n",
        "\n",
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓"
      ],
      "metadata": {
        "id": "EryHlYJvCRsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "world.book_to_world(book_path=\"/content/lotr1.txt\",\n",
        "                    text_splitter=TextSplitterTypes.SENTENCE_SPLITTER,\n",
        "                    max_units=3,\n",
        "                    overlap=1,\n",
        "                    encoding='utf-8')"
      ],
      "metadata": {
        "id": "ensgJmbtJDuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG: Checking which pieces of lore trigger our questions\n",
        "\n",
        "The lore from the book will be used to create NPCs questions. Let's see what Galadriel will retrieve with a simple question:\n",
        "\n",
        "`What do you think about the Rings of Power`?"
      ],
      "metadata": {
        "id": "_Cb335ANl4zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = world.get_lore(\"What do you think about the Rings of Power?\", num_results=5, min_similarity=0.95)\n",
        "for i, d in enumerate(results.documents):\n",
        "    print(f\"SENTENCE {i}:\\n{d}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xddh2hGCPapB",
        "outputId": "8e8b586d-10fa-467b-8524-d87351dbb113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SENTENCE 0:\n",
            "‘It  is  far  more  power¬ \n",
            "ful  than  I  ever  dared  to  think  at  first,  so  powerful  that  in  the \n",
            "end  it  would  utterly  overcome  anyone  of  mortal  race  who \n",
            "possessed  it.\n",
            "It  would  possess  him.\n",
            "‘In  Eregion  long  ago  many  Elven-rings  were  made,  magic \n",
            "rings  as  you  call  them,  and  they  were,  of  course,  of  various \n",
            "kinds:  some  more  potent  and  some  less.\n",
            "\n",
            "SENTENCE 1:\n",
            "These  Rings  have  a  way  of  being  found.\n",
            "\n",
            "SENTENCE 2:\n",
            "‘But  there  is  only  one \n",
            "Power  in  this  world  that  knows  all  about  the  Rings  and  their \n",
            "effects;  and  as  far  as  I  know  there  is  no  Power  in  the  world \n",
            "that  knows  all  about  hobbits.\n",
            "Among  the  Wise  I  am  the \n",
            "only  one  that  goes  in  for  hobbit-lore:  an  obscure  branch  of \n",
            "knowledge,  but  full  of  surprises.\n",
            "Soft  as  butter  they  can  be, \n",
            "and  yet  sometimes  as  tough  as  old  tree-roots.\n",
            "\n",
            "SENTENCE 3:\n",
            "Clearly  the  ring  had  an  unwholesome  power  that  set  to  work \n",
            "on  its  keeper  at  once.\n",
            "\n",
            "SENTENCE 4:\n",
            "The  lesser  rings  were \n",
            "only  essays  in  the  craft  before  it  was  full-grown,  and  to  the \n",
            "Elven-smiths  they  were  but  trifles  -  yet  still  to  my  mind \n",
            "dangerous  for  mortals.\n",
            "But  the  Great  Rings,  the  Rings  of \n",
            "Power,  they  were  perilous.\n",
            "‘A  mortal,  Frodo,  who  keeps  one  of  the  Great  Rings,  does \n",
            "not  die,  but  he  does  not  grow  or  obtain  more  life,  he  merely \n",
            "continues,  until  at  last  every  minute  is  a  weariness.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, `Spacy` did pretty well on understanding that these three lines...\n",
        "\n",
        "```\n",
        "‘It  is  far  more  power¬\n",
        "ful  than  I  ever  dared  to  think  at  first,  so  powerful  that  in  the\n",
        "end  it  would  utterly  overcome  anyone  of  mortal  race  who\n",
        "possessed  it.\n",
        "```\n",
        "\n",
        "are part of just one sentence! Good job, `spacy`!"
      ],
      "metadata": {
        "id": "w4HR1Hucmbto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's add Galadriel!\n",
        "Galadriel is ready to go east and join Middle Earth.\n",
        "\n",
        "But let's first define how she is!"
      ],
      "metadata": {
        "id": "jidF0ASAm5tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mindcraft.features.motivation import Motivation\n",
        "from mindcraft.features.personality import Personality\n",
        "from mindcraft.features.mood import Mood\n",
        "\n",
        "name = \"Galadriel\"\n",
        "description = \"The Elven Queen of Lothlorien, bearer of Nenya, wife to Celeborn\"\n",
        "personalities = [Personality(x) for x in ['fair', 'mighty', 'wise', 'carying', 'kind']]\n",
        "motivations = [Motivation(x) for x in ['Destroying the Evil', 'Protecting Middle Earth', 'Travelling West']]\n",
        "mood = Mood('worried')"
      ],
      "metadata": {
        "id": "2RrGrhE1QSGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `mighty`, `fair`, `wise` Galadriel, seeking to protect `Middle Earth` and `Destroying the evil`, feeling `worried` at the time , is ready to be added to the World."
      ],
      "metadata": {
        "id": "XIWagKlroDM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mindcraft.mind.npc import NPC\n",
        "\n",
        "galadriel = NPC(name,\n",
        "                description,\n",
        "                personalities,\n",
        "                motivations,\n",
        "                mood,\n",
        "                StoresTypes.CHROMA,\n",
        "                EmbeddingsTypes.MINILM)"
      ],
      "metadata": {
        "id": "P29OF2ilQdpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And last, let's add her to the World!"
      ],
      "metadata": {
        "id": "DmMIQZ47oujW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "galadriel.add_npc_to_world()"
      ],
      "metadata": {
        "id": "nF9tiuVLoyZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46624b0f-43a8-4794-ae85-0c9b7b288480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Galadriel now lives in <property object at 0x7ddc5cac55d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We have to talk..."
      ],
      "metadata": {
        "id": "saBy0OSEorwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, Galadriel is ready to have a `Zephyr7B`-based conversation, empowered by a `RAG`-based Middle Earth World. Sounds like fun but... does it work?"
      ],
      "metadata": {
        "id": "mEIu3UlVo7dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_iter = galadriel.react_to(\"What do you think about the Rings of Power\",\n",
        "                                min_similarity=0.85,\n",
        "                               ltm_num_results=3,\n",
        "                               world_num_results=7,\n",
        "                               max_tokens=600)"
      ],
      "metadata": {
        "id": "oxqEbfpyQnNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for answer in answer_iter:\n",
        "    print(answer)"
      ],
      "metadata": {
        "id": "xLK1-3XMebS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d6067a-0371-4b3b-d48b-d2b7f4ab8f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The One Ring was surely very powerful and dangerous.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "If you want to have better results using vLLM and fast inference, use Google Pro or another environment with at more than 16GB of GPU memory.\n",
        "\n",
        "That way, you will be able to use other state-of-the-art models as `Zephyr7B`, `Starling`, etc. included also in `mindcraft`."
      ],
      "metadata": {
        "id": "numdbbgApqZW"
      }
    }
  ]
}